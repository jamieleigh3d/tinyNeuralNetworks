import json
import csv
import os
import wx
import time
import threading
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from PIL import Image

domain_tag = 'domain_name'
main_image_id_tag = 'main_image_id'
item_id_tag = 'item_id'
item_name_tag = 'item_name'
image_id_tag = 'image_id'
height_tag = 'height'
width_tag = 'width'
path_tag = 'path'

listings_filepath = "data/abo/listings/metadata/listings_0.json"
imagedata_filepath = "data/abo/images/metadata/images.csv"

data = []

def filter(json_obj):
    if domain_tag in json_obj:
        return json_obj[domain_tag] == 'amazon.com'
    return False
   
with open(listings_filepath, 'r') as f:
    for line in f:
        json_obj = json.loads(line)
        if filter(json_obj):
            data.append(json_obj)

image_list = {}
with open (imagedata_filepath, newline='', encoding='utf-8') as csvfile:
    csv_reader = csv.DictReader(csvfile)
    for row in csv_reader:
        if image_id_tag in row:
            image_id = row[image_id_tag]
            image_list[image_id] = row

print(len(data))
if False:
    obj = data[0]

    item_id = obj.get(item_id_tag, "<UNK>")
    item_name = obj.get(item_name_tag, "[{}]")[0].get('value', "<UNK>")
    main_image_id = obj.get(main_image_id_tag, "<UNK>")
    print(item_id)
    print(item_name)
    print(main_image_id)

    main_image = image_list[main_image_id]
    print(main_image)


def get_filepath_for_object(obj, image_list):
    main_image_id = obj.get(main_image_id_tag, "<UNK>")
    main_image = image_list[main_image_id]
    image_folder = "data/abo/images/small"
    image_filepath = os.path.join(image_folder, main_image[path_tag])
    return image_filepath
    

# Generator
class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.label_emb = nn.Embedding(10, 10)
        self.model = nn.Sequential(
            nn.Linear(NOISE_DIM + 10 + NUM_DIFFUSION_STEPS, 256),
            nn.ReLU(),
            nn.Linear(256, 512),
            nn.ReLU(),
            nn.Linear(512, 784),
            nn.Tanh()
        )
        self.timestep_emb = nn.Embedding(NUM_DIFFUSION_STEPS, 10)  # Embedding for timesteps

    def forward(self, img, labels, timestep):
        c = self.label_emb(labels)
        t = self.timestep_emb(timestep)
        x = torch.cat([img, c, t], 1)
        return self.model(x)
        
class ImageLoaderFrame(wx.Frame):
    def __init__(self, parent, title):
        super().__init__(parent, title=title, size=(1200, 800))
        
        self.panel = wx.Panel(self)
        
        # Using a FlexGridSizer for the image grid
        self.grid = wx.FlexGridSizer(3, 3, 10, 10)
        self.grid.AddGrowableCol(0, 1)
        self.grid.AddGrowableCol(1, 1)
        self.grid.AddGrowableCol(2, 1)
        self.grid.AddGrowableRow(0, 1)
        self.grid.AddGrowableRow(1, 1)
        #self.grid.AddGrowableRow(2, 1)
        
        self.image_boxes = [wx.StaticBitmap(self.panel) for _ in range(9)]  # Create 9 image placeholders
        for box in self.image_boxes:
            self.grid.Add(box, flag=wx.EXPAND)
        
        self.vbox = wx.BoxSizer(wx.VERTICAL)
        self.vbox.Add(self.grid, proportion=1, flag=wx.ALL | wx.EXPAND, border=10)
        
        self.panel.SetSizer(self.vbox)


# Constants
BATCH_SIZE = 64
EPOCHS = 1
NOISE_DIM = 784
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Additional constants for diffusion
BETA = 0.01
NUM_DIFFUSION_STEPS = 30


# Initialize
G = Generator().to(DEVICE)
mse_loss = nn.MSELoss()
optimizer_g = optim.Adam(G.parameters(), lr=0.0002)

# Noise Schedule
def get_beta_for_step(step):
    # This is a constant noise schedule for simplification
    return BETA

# Diffusion Steps
def diffuse_image(image, num_steps):
    for step in range(num_steps):
        beta = get_beta_for_step(step)
        noise = torch.randn_like(image) * np.sqrt(beta * (1 - beta))
        image = image * (1 - beta) + noise
    return image

# Training Loop
def train_epoch(frame):
    for idx, obj in enumerate(data):
        
        batch_size = diffused_images.size(0)
        
        # Select a random number of timesteps to noisify the image, and train denoising
        timesteps_torch = torch.randint(0, NUM_DIFFUSION_STEPS, (batch_size,))  # sample random timesteps
        
        # Diffuse the real images
        
        denoised_images = G(diffused_images.view(batch_size,-1), labels.to(DEVICE), timesteps_torch.to(DEVICE))
        
        # Calculate MSE between the denoised images (generated by G) and the original images
        loss = mse_loss(denoised_images, images.to(DEVICE).view(batch_size,-1))
        
        # Backward and optimize the generator
        optimizer_g.zero_grad()
        loss.backward()
        optimizer_g.step()

    print(f"Epoch [{epoch + 1}/{EPOCHS}], g_loss: {loss.item():.4f}")

    # Visualization
    if epoch % 1 == 0:
        with torch.no_grad():
            labels = torch.LongTensor(np.arange(10)).to(DEVICE)
            
            final_denoised_images = []
            
            for i in range(10):  # Iterate over each label
                label = labels[i].unsqueeze(0)
                img = visualization_z[i].clone()
                
                for t in range(NUM_DIFFUSION_STEPS):  # Denoise over all timesteps
                    img = G(img.view(1,-1), label, torch.tensor([t]).to(DEVICE))  # Output is of shape (1, 784)
                    
                final_denoised_images.append(img.squeeze().cpu())  # Append the final image after all timesteps
                
            samples = torch.stack(final_denoised_images).view(-1, 28, 28)

        for i, ax in enumerate(display_images):
            ax.imshow(samples[i].numpy() * 0.5 + 0.5, cmap='gray')
            ax.axis('off')

        plt.draw()
        plt.pause(0.1)

def image_to_tensor(img):
    """Convert a PIL image to a PyTorch tensor."""
    np_img = np.array(img)
    tensor_img = torch.from_numpy(np_img).float().div(255)  # Convert image to [0, 1] range
    tensor_img = tensor_img.permute(2, 0, 1)  # Change dimensions to (C, H, W)
    return tensor_img

def tensor_to_image(tensor_img):
    """Convert a PyTorch tensor to a PIL image."""
    tensor_img = tensor_img.permute(1, 2, 0)  # Change dimensions to (H, W, C)
    np_img = (tensor_img.numpy() * 255).astype(np.uint8)  # Convert tensor to [0, 255] range
    return Image.fromarray(np_img)

def PIL_to_wxBitmap(pil_image):
    width, height = pil_image.size
    buffer = pil_image.convert("RGB").tobytes()
    wx_image = wx.Image(width, height, buffer)
    bitmap = wx_image.ConvertToBitmap()  # This converts it to a wx.Bitmap
    return bitmap

def show_image(frame, i, bitmap):
    #img = wx.Image(img_path, wx.BITMAP_TYPE_ANY)#.Scale(140, 140)  # Load and scale the image
    wx.CallAfter(frame.image_boxes[i].SetBitmap, bitmap)

def load_and_display_image(frame):
    
    #for epoch in range(EPOCHS):
    for steps in range(NUM_DIFFUSION_STEPS-1):
        for i, obj in enumerate(data[:6]):
            img_path = get_filepath_for_object(obj, image_list)
            pil_image = Image.open(img_path)
            image_tensor = image_to_tensor(pil_image)
            #train_epoch(frame)
            diffused_image_tensor = diffuse_image(image_tensor, steps).to(DEVICE)
            diffused_pil_image = tensor_to_image(diffused_image_tensor.cpu())
            
            # TODO: batch show_image() calls
            show_image(frame, i, PIL_to_wxBitmap(diffused_pil_image))
        #time.sleep(0.1)

def start_loading(frame):
    t = threading.Thread(target=load_and_display_image, args=(frame,))
    t.start()

if __name__ == "__main__":
    app = wx.App(False)
    frame = ImageLoaderFrame(None, 'Image Processing GUI')
    frame.Show()
    start_loading(frame)
    
    app.MainLoop()
    
print('Done!')